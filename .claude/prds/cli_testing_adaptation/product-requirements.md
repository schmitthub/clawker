# Product Requirements

> Generated by product-analyst for docker/cli - Testing Implementation Guide

## Product Overview

**Product Name:** Resource-Management CLI Testing Framework
**Product Type:** Testing infrastructure and methodology for a CLI that manages Docker-like resources
**Primary Purpose:** Enable comprehensive, multi-tiered testing of a command-line interface that creates, inspects, modifies, and deletes backend-managed resources -- without requiring the backend service for the majority of tests.

**Target Users:**
- **CLI developers**: Write and run unit tests rapidly during feature development, with zero external dependencies
- **Quality engineers**: Design end-to-end test suites that validate real workflows against live backends
- **CI/CD operators**: Configure automated pipelines that gate releases on test passage across multiple backend versions and platforms
- **Contributors**: Understand testing conventions quickly so they can add tests for new commands or fix regressions

---

## Core Features

### Feature 1: Mock CLI Infrastructure

**Description:** A mock implementation of the CLI application interface that captures all standard I/O to in-memory buffers, accepts a pluggable API client, and supports configuration/credential store injection. This enables tests to execute any CLI command in complete isolation -- no backend service, no filesystem side effects, no network calls.

**User Stories:**

##### US-1.1: Isolated Command Execution
> As a CLI developer, I want to execute any CLI command in a test without starting a backend service so that my test feedback loop is under one second.

**Acceptance Criteria:**
- [ ] Given a mock CLI wrapping a stub API client, when a command is executed with arguments, then it completes without network calls or filesystem writes outside the test sandbox
- [ ] Given a mock CLI, when a command writes to stdout or stderr, then the output is captured in readable buffers accessible via `OutBuffer()` and `ErrBuffer()` methods
- [ ] Given a mock CLI, when a command reads from stdin (e.g., confirmation prompts), then simulated input can be injected before execution via `SetIn()`
- [ ] Edge case: When no API methods are configured on the stub client, the command receives safe zero-value defaults (empty lists, nil errors) rather than panicking

##### US-1.2: Output Assertion
> As a CLI developer, I want to inspect captured stdout and stderr after command execution so that I can verify formatting, content, and error messages.

**Acceptance Criteria:**
- [ ] Given a command has executed, when `OutBuffer().String()` is called, then it returns the exact stdout produced by the command
- [ ] Given a command has executed, when `ErrBuffer().String()` is called, then it returns the exact stderr produced by the command
- [ ] Given multiple commands are executed sequentially in one test, when `ResetOutputBuffers()` is called between executions, then buffers are cleared for the next assertion

##### US-1.3: Configuration and Context Injection
> As a CLI developer, I want to inject configuration files, credential stores, and endpoint contexts into the mock CLI so that I can test commands that depend on user configuration.

**Acceptance Criteria:**
- [ ] Given a mock CLI, when `SetConfigFile()` is called with test configuration, then commands read from that configuration
- [ ] Given a mock CLI, when `SetCurrentContext()` is called, then commands use the specified context for backend selection
- [ ] Given a mock CLI, when a Docker endpoint or content trust configuration is injected, then commands behave as if those features are configured

---

### Feature 2: Function-Field API Client Stubs

**Description:** A mocking pattern where each resource domain (containers, images, networks, volumes, services, etc.) has a stub client type with overridable function fields for every API method. Tests declare only the methods relevant to their scenario; all other methods return safe defaults. No code generation, reflection, or external mocking libraries are used.

**User Stories:**

##### US-2.1: Per-Test API Behavior
> As a CLI developer, I want to specify custom return values for specific API methods on a per-test basis so that I can simulate any backend response scenario.

**Acceptance Criteria:**
- [ ] Given a stub client with a function field set for `ContainerList`, when the command under test calls `ContainerList`, then the function field is invoked and its return value is used
- [ ] Given a stub client with no function field set for `ContainerList`, when the command calls `ContainerList`, then zero values are returned (empty slice, nil error)
- [ ] Given a stub client, when a test sets multiple function fields (e.g., `ContainerCreate` and `ContainerStart`), then each method uses its respective function field independently

##### US-2.2: Error Injection
> As a CLI developer, I want to inject specific error types (not-found, permission-denied, conflict, invalid-parameter) from API method stubs so that I can test all error-handling paths.

**Acceptance Criteria:**
- [ ] Given a function field that returns a "not found" typed error, when the command processes it, then it produces the appropriate user-facing error message
- [ ] Given a function field that returns a "conflict" typed error, when the command processes it, then it distinguishes it from other error types
- [ ] Given a function field that returns a generic error string, when the command processes it, then `assert.ErrorContains(t, err, "expected substring")` validates the message

##### US-2.3: Guard Functions for Unexpected Calls
> As a CLI developer, I want to detect when an API method is called unexpectedly during a test so that I can catch logic errors in command implementation.

**Acceptance Criteria:**
- [ ] Given a function field set to call `t.Fatalf("unexpected call")`, when the command invokes that method, then the test fails immediately with a descriptive message
- [ ] Given a guard function on `ContainerRemove`, when a "list-only" command is tested and does not call remove, then the guard is never triggered and the test passes

##### US-2.4: Per-Resource Domain Isolation
> As a CLI developer, I want each resource domain (containers, images, networks, volumes, services, secrets, configs, nodes, system) to have its own stub client so that tests only declare methods relevant to their domain.

**Acceptance Criteria:**
- [ ] Given a container command test, when a container-domain stub client is created, then only container-related function fields are present
- [ ] Given 19+ resource domains in the CLI, when each domain has a stub client, then no single "god mock" accumulates all methods
- [ ] Given a stub client embeds the real API client interface, when a method without a function field is called, then the embedded interface's nil implementation is used safely

---

### Feature 3: Test Data Builder System

**Description:** A library of builder functions using the functional-options pattern to construct complex resource objects (containers, networks, volumes, services, tasks, nodes, configs, secrets) for test assertions. Builders provide composable, readable test data without manual struct initialization.

**User Stories:**

##### US-3.1: Composable Test Object Construction
> As a CLI developer, I want to construct test resource objects (e.g., a container with specific labels, ports, and status) using a fluent builder API so that test data is readable and maintainable.

**Acceptance Criteria:**
- [ ] Given a container builder called with `Container("web", WithLabel("app","nginx"), WithPort(80,8080,"tcp"))`, then the resulting object has the specified name, label, and port mapping
- [ ] Given a builder called with no options, then sensible defaults are applied (e.g., auto-generated ID from name, default empty maps)
- [ ] Given multiple builders for different resource types (container, network, volume, service, task, node, config, secret), then all follow the same functional-options pattern

##### US-3.2: Test Data for Formatter Validation
> As a CLI developer, I want to build collections of resource objects to feed into output formatters so that I can verify table, JSON, and template rendering.

**Acceptance Criteria:**
- [ ] Given a slice of containers built with varying properties, when passed to a list formatter, then the formatter produces correctly aligned table output
- [ ] Given builders that set all fields exercised by a formatter (ID, name, status, ports, labels, size), then golden file comparisons catch formatting regressions

---

### Feature 4: Golden File Snapshot Testing

**Description:** A workflow for comparing command output against version-controlled reference files (golden files). When output intentionally changes, a single environment variable (`UPDATE_GOLDEN=1`) regenerates all golden files for review and commit. This covers table output, JSON output, custom template output, and error messages.

**User Stories:**

##### US-4.1: Output Regression Detection
> As a CLI developer, I want command output to be compared against committed reference files so that unintended formatting changes are caught during testing.

**Acceptance Criteria:**
- [ ] Given a golden file `container-list-default.golden` exists in `testdata/`, when a list command produces output, then it is compared byte-for-byte against the golden file
- [ ] Given the output differs from the golden file, then the test fails with a diff showing the exact differences
- [ ] Given 100+ golden files across the codebase, then each is stored alongside its test in a `testdata/` subdirectory

##### US-4.2: Golden File Update Workflow
> As a CLI developer, I want to regenerate golden files after intentional output changes so that I can review and commit the new baselines.

**Acceptance Criteria:**
- [ ] Given `UPDATE_GOLDEN=1` is set in the environment, when tests run, then golden files are overwritten with current output
- [ ] Given golden files are updated, when `git diff testdata/` is run, then all changes are visible for review before commit
- [ ] Given a specific test is run with `UPDATE_GOLDEN=1 go test ./path/... -run TestName`, then only that test's golden files are updated

---

### Feature 5: Table-Driven Test Organization

**Description:** A standardized pattern for organizing related test cases as slices of anonymous structs with descriptive `name` fields, executed as subtests via `t.Run()`. This pattern covers happy paths, error cases, edge cases, and flag validation in a single, scannable test function.

**User Stories:**

##### US-5.1: Comprehensive Case Coverage
> As a CLI developer, I want to express 3-10 related test scenarios in a single test function so that flag validation, error paths, and edge cases are co-located and easy to extend.

**Acceptance Criteria:**
- [ ] Given a table-driven test with a `name`, `args`, and `expectedErr` field per case, when each case runs as a subtest via `t.Run(tc.name, ...)`, then failures identify the exact failing case
- [ ] Given a new edge case is discovered, when a developer adds a struct entry to the table, then no boilerplate setup/teardown code needs duplication
- [ ] Given subtests run independently, when one case fails, then remaining cases still execute and report their results

##### US-5.2: Self-Documenting Test Names
> As a contributor, I want each test case to have a descriptive lowercase name so that test output reads naturally (e.g., "TestRunOptions/no_image_specified").

**Acceptance Criteria:**
- [ ] Given subtest names like "conflicting options" or "invalid memory format", when test output is printed, then the name clearly describes the scenario without reading the code
- [ ] Given the naming convention `Test<Subject>[<Case>]` for top-level tests and descriptive lowercase for subtests, then all test files follow this convention consistently

---

### Feature 6: End-to-End Test Framework

**Description:** A full integration test tier that executes the compiled CLI binary as a subprocess against real backend services (daemon, registry, orchestrator) provisioned by container orchestration. Tests validate real workflows including image push/pull, container lifecycle, network creation, and plugin interaction.

**User Stories:**

##### US-6.1: Real Binary Execution
> As a quality engineer, I want to execute the actual CLI binary against a live backend so that integration issues between the CLI and the backend API are detected.

**Acceptance Criteria:**
- [ ] Given the CLI binary is compiled and available, when `icmd.RunCommand("docker", "run", "--rm", "alpine", "echo", "hello")` is called, then it executes the real binary and captures stdout, stderr, and exit code
- [ ] Given a command result, when `result.Assert(t, icmd.Expected{ExitCode: 0, Out: "hello"})` is called, then all three dimensions (exit code, stdout, stderr) are validated
- [ ] Given the E2E test directory structure mirrors the source structure (`e2e/container/`, `e2e/image/`, etc.), then tests are discoverable by resource type

##### US-6.2: Service Environment Provisioning
> As a quality engineer, I want E2E tests to automatically provision backend services (daemon, registry) using container orchestration so that tests are reproducible and self-contained.

**Acceptance Criteria:**
- [ ] Given a Compose file defines a backend daemon service and a registry service, when E2E tests start, then both services are running and accessible
- [ ] Given the registry runs on port 5000 with insecure mode, when image push/pull tests execute, then they operate against the local registry without TLS
- [ ] Given setup scripts initialize a swarm cluster, when swarm service tests execute, then they have a valid orchestrator environment
- [ ] Given cleanup runs after tests, when Compose teardown executes, then all volumes and networks are removed

##### US-6.3: Backend Version Matrix Testing
> As a CI/CD operator, I want E2E tests to run against multiple backend versions so that backward compatibility is validated before release.

**Acceptance Criteria:**
- [ ] Given a CI matrix of 4 backend versions (latest RC, N, N-1, N-4), when E2E tests execute, then they run independently against each version
- [ ] Given 2 base image variants (Alpine, Debian) and 2 connection types (local socket, SSH tunnel), then the full matrix is 16 combinations (4 x 2 x 2)
- [ ] Given a test requires features only available in newer backends, when a skip condition checks the API version, then the test is skipped on older versions with a clear reason

##### US-6.4: Conditional Test Skipping
> As a quality engineer, I want tests to skip automatically based on platform capabilities so that tests do not fail spuriously on unsupported environments.

**Acceptance Criteria:**
- [ ] Given `skip.If(t, environment.RemoteDaemon())`, when the daemon is remote (e.g., SSH), then the test is skipped with a message
- [ ] Given `skip.If(t, !environment.DaemonIsLinux())`, when the daemon runs on a non-Linux OS, then Linux-only tests are skipped
- [ ] Given `skip.If(t, versions.LessThan(apiVersion, "1.40"))`, when the backend API is older than required, then the test is skipped
- [ ] Given skip conditions are evaluated at test start, when a test is skipped, then no backend calls are made

---

### Feature 7: Asynchronous and Concurrent Test Support

**Description:** Patterns for testing operations that involve concurrency (container attach, signal forwarding, streaming output) using channels with timeout guards, polling loops for eventual consistency, and write-hook synchronization for output timing.

**User Stories:**

##### US-7.1: Channel-Based Async Testing
> As a CLI developer, I want to test commands that block (e.g., attach, logs --follow) using goroutines with timeout guards so that tests do not hang indefinitely on failure.

**Acceptance Criteria:**
- [ ] Given a command runs in a goroutine writing to a `chan error`, when the command completes, then the result is received via `select` with a timeout
- [ ] Given the command hangs, when the timeout (e.g., 10 seconds) expires, then `t.Fatal("timeout")` is called and the test fails with a clear message
- [ ] Given a buffered channel of size 1 is used, when the goroutine completes before the receiver reads, then no goroutine leak occurs

##### US-7.2: Polling for Eventual Consistency
> As a quality engineer, I want to poll for asynchronous state changes (e.g., container becomes "running") in E2E tests so that tests handle backend timing variability.

**Acceptance Criteria:**
- [ ] Given `poll.WaitOn(t, check, poll.WithTimeout(30*time.Second))`, when the check function returns `poll.Success()`, then polling stops and the test continues
- [ ] Given the check function returns `poll.Continue("reason")`, when the timeout has not expired, then the check is retried after a configurable delay
- [ ] Given the timeout expires without success, then `t.Fatal()` is called with the last continue reason

##### US-7.3: Write-Hook Output Synchronization
> As a CLI developer, I want to synchronize test assertions with async output writes so that assertions run only after expected output has been produced.

**Acceptance Criteria:**
- [ ] Given a writer hook is attached to the mock CLI's output stream, when the command writes any output, then a channel is signaled
- [ ] Given the channel is signaled, when the test reads from the channel, then it can safely assert on the output buffer contents
- [ ] Given no output is written within the timeout, then the test fails with "timeout waiting for output"

---

### Feature 8: Assertion and Validation Library

**Description:** A unified assertion library providing equality checks, error inspection, collection assertions, deep comparison, and golden file comparison. All tests use a single assertion framework with no mixing of assertion libraries.

**User Stories:**

##### US-8.1: Consistent Assertion API
> As a CLI developer, I want a single assertion library used across all tests so that assertion patterns are predictable and learnable.

**Acceptance Criteria:**
- [ ] Given the assertion library provides `assert.NilError`, `assert.Equal`, `assert.ErrorContains`, `assert.Check`, and `assert.Assert`, then all unit and E2E tests use only these functions
- [ ] Given comparison helpers like `is.Equal`, `is.Len`, `is.Contains`, and `is.DeepEqual`, then complex assertions compose naturally
- [ ] Given `assert.Check` continues on failure while `assert.Assert` stops the test, then developers choose the appropriate severity
- [ ] Given deep equality via `is.DeepEqual` supports comparison options (e.g., ignore empty collections), then struct comparisons are precise

##### US-8.2: Error Type Validation
> As a CLI developer, I want to assert on specific error types and messages so that error-handling code paths are verified.

**Acceptance Criteria:**
- [ ] Given `assert.ErrorContains(t, err, "permission denied")`, when the error message contains the substring, then the assertion passes
- [ ] Given `errors.As(err, &statusErr)`, when the error is a status error, then `statusErr.StatusCode` can be checked for specific HTTP-level codes
- [ ] Given a typed error (not-found, conflict, forbidden), when the test checks the error category, then it validates the error classification, not just the message string

---

### Feature 9: Test Execution and Runner Infrastructure

**Description:** A containerized test execution system using multi-stage container builds for reproducible environments. Includes an enhanced test runner with formatted output, coverage collection, and parallel execution support.

**User Stories:**

##### US-9.1: Containerized Test Execution
> As a CI/CD operator, I want all tests to run inside containers so that results are reproducible regardless of the host environment.

**Acceptance Criteria:**
- [ ] Given a build target `test` is defined, when `docker buildx bake test` is executed, then unit tests run in a container with all dependencies pre-installed
- [ ] Given a build target `test-coverage` is defined, when executed, then coverage data is written to `build/coverage/coverage.txt`
- [ ] Given the test container mounts Go module and build caches, when tests run repeatedly, then subsequent runs benefit from cached compilation

##### US-9.2: Local Development Workflow
> As a CLI developer, I want to run tests locally with minimal setup so that I can iterate quickly during development.

**Acceptance Criteria:**
- [ ] Given Go is installed locally, when `make test-unit` is run, then unit tests execute directly without Docker
- [ ] Given `gotestsum` is installed, when `gotestsum -- ./path/... -run TestName` is run, then a single test executes with formatted output
- [ ] Given a development container is available, when `make shell` is run, then an interactive shell opens with all test tools pre-installed

##### US-9.3: Coverage Collection and Reporting
> As a CI/CD operator, I want test coverage to be collected and uploaded automatically so that coverage trends are tracked over time.

**Acceptance Criteria:**
- [ ] Given unit tests generate `coverage.txt`, when uploaded to a coverage service, then per-file and per-function coverage is visible
- [ ] Given E2E tests also generate coverage profiles, when uploaded, then both tiers contribute to overall coverage metrics
- [ ] Given patch coverage threshold is 50%, when a PR reduces coverage below threshold, then the coverage check fails
- [ ] Given test infrastructure files, vendor code, and generated files are excluded, then coverage metrics reflect only production code

---

### Feature 10: Code Quality Gates

**Description:** A suite of automated quality checks including static analysis (40+ lint rules), code formatting enforcement, shell script validation, vendor directory integrity, and documentation generation verification. All gates run in CI before merge.

**User Stories:**

##### US-10.1: Static Analysis
> As a CI/CD operator, I want automated linting to catch code quality issues before merge so that the codebase maintains consistent quality.

**Acceptance Criteria:**
- [ ] Given a lint configuration with 40+ rules, when `docker buildx bake lint` is run, then all source files are analyzed
- [ ] Given formatting uses a strict formatter (stricter than standard `gofmt`), when code is not formatted, then the lint check fails
- [ ] Given shell scripts exist in `scripts/` and `contrib/`, when `docker buildx bake shellcheck` is run, then all scripts are validated

##### US-10.2: Vendor and Documentation Integrity
> As a CI/CD operator, I want vendor directory and generated documentation to be verified so that out-of-sync artifacts are caught.

**Acceptance Criteria:**
- [ ] Given vendor dependencies are committed, when the validate-vendor target runs, then any missing or extra vendored files cause failure
- [ ] Given documentation is auto-generated from command definitions, when the docs-generation target runs and detects a diff, then the check fails

---

## Supporting Features

### Terminal/PTY Testing
For commands requiring terminal emulation (interactive attach, signal proxying, window resize), a PTY library enables tests to open pseudo-terminals, send input, and read output as if a real terminal were connected. Tests using PTY are platform-specific (Unix-only) and are skipped on unsupported platforms.

### Environment Variable Patching
Tests that depend on environment variables (e.g., `DOCKER_HOST`, `HOME`, `DOCKER_CONFIG`) use a patching utility that sets variables for the test duration and restores originals on cleanup via `t.Cleanup()`.

### Filesystem Utilities
Temporary directory and file creation with automatic cleanup. Structured directory trees can be created declaratively (e.g., a temp directory containing a `config.json` file with specific content).

### Context Cancellation Testing
Commands that support graceful shutdown via context cancellation can be tested by creating a cancellable context, starting the command, and cancelling the context to verify the command returns `context.Canceled`.

### Signal Handling Verification
Unix signal forwarding (SIGTERM, SIGWINCH, SIGKILL) is tested at both the unit level (signal channels) and E2E level (process signal delivery to containerized processes via PTY).

---

## Non-Functional Requirements

### Performance

| Metric | Requirement | Notes |
|--------|-------------|-------|
| Unit test suite execution | Under 2 minutes | No network or daemon dependency |
| Single unit test execution | Under 1 second | Mock CLI with in-memory buffers |
| E2E suite execution (single matrix cell) | Under 15 minutes | Includes service provisioning |
| CI pipeline total (all tiers) | Under 30 minutes | Parallel matrix execution |

### Reliability

- Unit tests must be fully deterministic: no flakiness from timing, ordering, or environment
- E2E tests use polling with configurable timeouts rather than fixed sleeps to handle backend timing variability
- Channel-based async tests include timeout guards (typically 5-10 seconds) to prevent indefinite hangs
- E2E cleanup runs unconditionally (setup -> test -> cleanup pattern) to prevent resource leaks across test runs

### Scalability

- The testing framework must support 150+ unit test files and 30+ E2E test files without architectural changes
- Adding a new CLI command requires only: (1) a stub client with relevant function fields, (2) test functions following the established pattern, (3) golden files in a `testdata/` directory
- The E2E matrix must support expansion to additional backend versions and platform variants by adding entries to the CI configuration

### Security

- E2E test environments use insecure registries (no TLS) intentionally for simplicity; this must never be applied to production configurations
- Test credential stores use in-memory or temporary file-based storage; real credentials are never embedded in tests
- Coverage reports and test artifacts do not contain sensitive data and are safe for upload to external services
- CI secrets (coverage tokens, registry credentials) are managed via the CI platform's secret store

### Usability

- A new contributor must be able to run unit tests with a single command (`make test-unit` or `docker buildx bake test`)
- The mock CLI pattern must be learnable from a single example: create stub, wrap in mock CLI, create command, set args, execute, assert
- Golden file updates must be a single-step process (`UPDATE_GOLDEN=1 go test ./...`)
- Test failures must identify the exact test case, expected value, and actual value with a diff

### Compatibility

- Unit tests must pass on macOS (ARM64, Intel) and Linux (amd64) without modification
- E2E tests target Linux containers; Windows testing is documented as out of scope (skipped in CI)
- The framework must support testing against 4 concurrent backend versions (latest RC through N-4)
- Container-based test execution must work with Docker BuildKit (buildx bake)

---

## Assumptions

1. The CLI under test follows a command-handler architecture where each command is a function that receives a CLI interface and returns an error
2. The backend API is accessed through a well-defined client interface that can be stubbed at the interface level
3. All CLI output goes through the CLI interface's stdout/stderr streams (not directly to `os.Stdout`)
4. Backend services for E2E testing can be provisioned as containers using Docker Compose
5. The Go programming language is used, with its standard `testing` package as the foundation
6. A single assertion library is adopted project-wide and enforced by convention
7. Golden files are committed to version control and reviewed as part of code review
8. CI runners have Docker installed and can run privileged containers (for Docker-in-Docker)

---

## Constraints

1. **No external mocking libraries:** The function-field pattern is used exclusively; no gomock, testify/mock, or mockery
2. **No code generation for mocks:** Stub clients are hand-written, which means ~19 per-resource stub files with similar boilerplate
3. **E2E tests require privileged containers:** Docker-in-Docker requires `--privileged` mode, which limits where E2E tests can run
4. **Platform-specific tests are Unix-only:** Signal handling and PTY tests cannot run on Windows
5. **Golden files are platform-sensitive:** Some golden files may produce different output on different platforms (line endings, path separators)
6. **No formal integration tier:** The gap between unit tests (fully mocked) and E2E tests (full binary + live backend) is by design; no intermediate tier exists

---

## Out of Scope

1. **Windows E2E testing:** Currently commented out in CI; tests skip on Windows platforms
2. **Performance/load testing:** No benchmarks or load tests are defined
3. **Fuzz testing:** No fuzzing infrastructure, though Go supports it natively
4. **Visual/UI testing:** CLI output is text-only; no visual regression testing
5. **Mock consolidation:** The 19 per-resource stub clients are intentionally not consolidated into a single shared mock
6. **Integration test tier:** A tier between unit and E2E is explicitly not implemented; the project relies on the two-tier model

---

## Dependencies

1. **Go standard library `testing` package:** Foundation for all test execution
2. **gotest.tools/v3:** Primary assertion, golden file, polling, skip, filesystem, and command execution library
3. **google/go-cmp:** Deep comparison with struct field control, used via `is.DeepEqual`
4. **creack/pty:** Pseudo-terminal operations for interactive command testing (Unix-only)
5. **gotestsum:** Enhanced test runner with formatted output and coverage collection
6. **Docker BuildKit (buildx):** Container-based test execution and caching
7. **Docker Compose:** E2E service environment provisioning
8. **Codecov:** Coverage reporting and trend tracking
9. **golangci-lint:** Static analysis with 40+ lint rules
10. **shellcheck:** Shell script validation

---

## Glossary

| Term | Definition |
|------|------------|
| Mock CLI | An implementation of the CLI application interface that captures I/O to buffers and accepts a pluggable API client |
| Stub client | A per-resource-domain type with overridable function fields for each API method |
| Function-field pattern | A mocking technique where struct fields hold functions that are called by interface method implementations; nil fields return zero values |
| Golden file | A version-controlled reference file containing expected command output, used for snapshot testing |
| Builder | A function using the functional-options pattern to construct complex test data objects |
| Table-driven test | A Go testing pattern where test cases are defined as a slice of structs and iterated with `t.Run()` subtests |
| E2E matrix | The combinatorial set of backend versions, base images, and connection types tested in CI |
| Docker-in-Docker (DinD) | Running a Docker daemon inside a Docker container, used to provision isolated backend environments for E2E tests |
| Polling | Repeatedly checking a condition with configurable timeout and delay until it succeeds or times out |
| Guard function | A function-field stub that calls `t.Fatalf()` if invoked, detecting unexpected API calls |
| Write hook | A writer wrapper that signals a channel when output is written, enabling synchronization in async tests |

---

## Feature Priority Matrix

| Feature | Priority | Complexity | Notes |
|---------|----------|------------|-------|
| Mock CLI Infrastructure | Must Have | Medium | Foundation for all unit testing; implement first |
| Function-Field API Client Stubs | Must Have | Medium | One stub per resource domain; boilerplate but essential |
| Assertion and Validation Library | Must Have | Low | Adopt a single library; enforce by convention |
| Table-Driven Test Organization | Must Have | Low | Convention, not code; enforce via code review |
| Test Data Builder System | Should Have | Medium | Invest early; pays off as resource types grow |
| Golden File Snapshot Testing | Should Have | Low | Simple implementation, high regression-detection value |
| End-to-End Test Framework | Should Have | High | Requires container orchestration infrastructure |
| Test Execution and Runner Infrastructure | Should Have | High | Containerized builds, caching, CI integration |
| Asynchronous and Concurrent Test Support | Should Have | Medium | Needed for streaming/attach commands |
| Code Quality Gates | Should Have | Medium | Linting, formatting, vendor validation |
| Terminal/PTY Testing | Nice to Have | Medium | Unix-only; needed for interactive commands |
| Signal Handling Verification | Nice to Have | Medium | Unix-only; platform-specific |
| Context Cancellation Testing | Nice to Have | Low | Standard Go pattern; add as needed |
